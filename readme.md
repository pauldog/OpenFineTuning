Intro
======

The idea is to create a human currated set of question/answer pairs for fine tuning a LLM which is opensource and copyright free.

The fine tuning teaches an AI what the humans would like as an answer in response to questions.

To add new questions to the training data, the best place to add them is the Wiki page.

For reference the Alpaca traing set is [here](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)

There are lots of different ways to fine tune a model. For example, could we fine tune it to be good at tic-tac-toe?

Each fine tuning can be thought of as an intensive course that the AI is studying. Therefor there could be arguments for splitting the fine training data into different categories.

Useful Links
============
These are some useful data you could use for training:

[Multiple Choice json](https://gist.github.com/cmota/f7919cd962a061126effb2d7118bec72)

[Health Forum json](https://github.com/LasseRegin/medical-question-answer-data/blob/master/ehealthforumQAs.json)

[First lines of novels](https://github.com/janelleshane/novel-first-lines-dataset) Useful to train a story writing LLM?

[Open Assistant](https://open-assistant.io/) A similar open source project
